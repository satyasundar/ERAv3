{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/satyasundar/ERAv3/blob/colab/era_s6_base_file.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZ6JRn-VnAC3"
   },
   "source": [
    "### Different techniques used in writing new model\n",
    "1. How many layers,\n",
    "2. MaxPooling,\n",
    "3. 1x1 Convolutions,\n",
    "4. 3x3 Convolutions,\n",
    "5. Receptive Field,\n",
    "6. SoftMax,\n",
    "7. Learning Rate,\n",
    "8. Kernels and how do we decide the number of kernels?\n",
    "9. Batch Normalization,\n",
    "10. Image Normalization,\n",
    "11. Position of MaxPooling,\n",
    "12. Concept of Transition Layers,\n",
    "13. Position of Transition Layer,\n",
    "14. DropOut\n",
    "15. When do we introduce DropOut, or when do we know we have some overfitting\n",
    "16. The distance of MaxPooling from Prediction,\n",
    "17. The distance of Batch Normalization from Prediction,\n",
    "18. When do we stop convolutions and go ahead with a larger kernel or some other alternative (which we have not yet covered)\n",
    "19. How do we know our network is not going well, comparatively, very early\n",
    "20. Batch Size, and Effects of Batch Size\n",
    "21. etc (you can add more if we missed it here)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzVTYNZVmQG7"
   },
   "source": [
    "## WRITE THE MODEL AGAIN SUCH THAT IT ACHIEVES\n",
    "1. 99.4% validation/test accuracy (50/10k split, basically we are calling Validation dataset as test dataset itself)\n",
    "2. Less than 20k Parameters\n",
    "3. You can use anything from above you want.\n",
    "4. Less than 20 Epochs\n",
    "5. Have used BN, Dropout,\n",
    "6. (Optional): a Fully connected layer or, have used GAP.\n",
    "7. To learn how to add different things we covered in this session, you can refer to this code: https://www.kaggle.com/enwei26/mnist-digits-pytorch-cnn-99 DONT COPY ARCHITECTURE, JUST LEARN HOW TO INTEGRATE THINGS LIKE DROPOUT, BATCHNORM, ETC.\n",
    "8. **Someone has achieved 99.6% in 3400 parameters in 6 epochs**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LEYhCwd3zZkm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\n",
      "Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "0m2JWFliFfKT"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QtT8m9sAxK3W",
    "outputId": "b48f20e0-0673-4f28-84e5-a6bfdb022b29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "Total Parameters : 10458\n",
      "Network(\n",
      "  (conv3): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (transition1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv5): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (gap): AdaptiveAvgPool2d(output_size=1)\n",
      "  (fc_gap): Linear(in_features=32, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# My MNIST model training\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        #FC model\n",
    "        # self.conv1   = nn.Conv2d(1, 8, 3, padding=1)\n",
    "        # self.conv2   = nn.Conv2d(8, 8, 3, padding=1)\n",
    "        # self.fc1    = nn.Linear(8*7*7, 60)\n",
    "        # self.fc2    = nn.Linear(60, 10)\n",
    "\n",
    "        # GAP model\n",
    "        self.conv3 = nn.Conv2d(1, 16, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.transition1 = nn.Conv2d(32, 16, 1)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # self.conv6 = nn.Conv2d(32, 48, 3, padding=1)\n",
    "        # self.bn4 = nn.BatchNorm2d(48)\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc_gap = nn.Linear(32, 10)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #FC\n",
    "        # x = self.conv1(x)\n",
    "        # x = self.bn8(x)\n",
    "        # x = F.relu(x)\n",
    "        # x = self.pool(x)\n",
    "        # x = self.dropout(x)\n",
    "\n",
    "        # x = self.conv2(x)\n",
    "        # x = self.bn8(x)\n",
    "        # x = F.relu(x)\n",
    "        # x = self.pool(x)\n",
    "        # x = self.dropout(x)\n",
    "\n",
    "        # x = self.pool(self.relu(self.conv1(x)))\n",
    "        # x = self.pool(self.relu(self.conv2(x)))\n",
    "        # x = x.view(-1, 8*7*7)\n",
    "        # x = self.fc1(x)\n",
    "        # x = F.relu(x)\n",
    "        # x = self.fc2(x)\n",
    "\n",
    "        #GAP\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        #x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        #x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transition1(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # x = self.conv6(x)\n",
    "        # x = self.bn4(x)\n",
    "        # x = F.relu(x)\n",
    "        \n",
    "\n",
    "        x = self.gap(x)\n",
    "        x = x.view(-1, 32)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc_gap(x)\n",
    "        \n",
    "        #return x\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "#device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = Network().to(device)\n",
    "#summary(model, input_size=(1, 28,28))\n",
    "\n",
    "network = Network()\n",
    "total_parameters = sum(p.numel() for p in network.parameters())\n",
    "print(f\"Total Parameters : {total_parameters}\")\n",
    "print(network)\n",
    "\n",
    "\n",
    "# for name, param in network.named_parameters():\n",
    "#     print(name, '\\t\\t', param.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eQjSz9vSy7_n",
    "outputId": "49399273-7435-4fe5-df35-3c82a72ce321"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/469 [00:00<?, ?it/s]/var/folders/w9/1gz_7_dn4zs29tysldrxvfnh0000gn/T/ipykernel_36137/2901795345.py:85: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n",
      "loss=0.7879770398139954 batch_id=468: 100%|██████████| 469/469 [00:21<00:00, 22.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.5983, Accuracy: 8285/10000 (82.8%)\n",
      "\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.5502829551696777 batch_id=468: 100%|██████████| 469/469 [00:20<00:00, 22.64it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.4095, Accuracy: 8513/10000 (85.1%)\n",
      "\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.5713484883308411 batch_id=468: 100%|██████████| 469/469 [00:20<00:00, 22.59it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2537, Accuracy: 9329/10000 (93.3%)\n",
      "\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.7417109608650208 batch_id=468: 100%|██████████| 469/469 [00:20<00:00, 22.63it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2255, Accuracy: 9369/10000 (93.7%)\n",
      "\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.5502129793167114 batch_id=468: 100%|██████████| 469/469 [00:20<00:00, 22.34it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2047, Accuracy: 9411/10000 (94.1%)\n",
      "\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.3893090486526489 batch_id=468: 100%|██████████| 469/469 [00:21<00:00, 21.35it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1867, Accuracy: 9513/10000 (95.1%)\n",
      "\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.2576674520969391 batch_id=468: 100%|██████████| 469/469 [00:21<00:00, 22.26it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1621, Accuracy: 9552/10000 (95.5%)\n",
      "\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.44188567996025085 batch_id=468: 100%|██████████| 469/469 [00:20<00:00, 22.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1858, Accuracy: 9445/10000 (94.5%)\n",
      "\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.4413597583770752 batch_id=468: 100%|██████████| 469/469 [00:20<00:00, 22.71it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1743, Accuracy: 9526/10000 (95.3%)\n",
      "\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.23638950288295746 batch_id=468: 100%|██████████| 469/469 [00:20<00:00, 22.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1567, Accuracy: 9553/10000 (95.5%)\n",
      "\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.3465500771999359 batch_id=468: 100%|██████████| 469/469 [00:20<00:00, 22.78it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1786, Accuracy: 9493/10000 (94.9%)\n",
      "\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.2573040723800659 batch_id=468: 100%|██████████| 469/469 [00:20<00:00, 22.75it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1479, Accuracy: 9549/10000 (95.5%)\n",
      "\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.26747265458106995 batch_id=468: 100%|██████████| 469/469 [00:20<00:00, 22.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1296, Accuracy: 9615/10000 (96.2%)\n",
      "\n",
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.38857707381248474 batch_id=468: 100%|██████████| 469/469 [00:20<00:00, 22.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1344, Accuracy: 9614/10000 (96.1%)\n",
      "\n",
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.22162210941314697 batch_id=468: 100%|██████████| 469/469 [00:20<00:00, 22.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1393, Accuracy: 9617/10000 (96.2%)\n",
      "\n",
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.3512742817401886 batch_id=468: 100%|██████████| 469/469 [00:20<00:00, 22.88it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1244, Accuracy: 9665/10000 (96.7%)\n",
      "\n",
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.4225713908672333 batch_id=468: 100%|██████████| 469/469 [00:20<00:00, 22.86it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1247, Accuracy: 9639/10000 (96.4%)\n",
      "\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.3747236728668213 batch_id=468: 100%|██████████| 469/469 [00:20<00:00, 22.82it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1329, Accuracy: 9609/10000 (96.1%)\n",
      "\n",
      "18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.35992392897605896 batch_id=468: 100%|██████████| 469/469 [00:20<00:00, 22.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1272, Accuracy: 9643/10000 (96.4%)\n",
      "\n",
      "19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.23400455713272095 batch_id=468: 100%|██████████| 469/469 [00:20<00:00, 22.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1322, Accuracy: 9623/10000 (96.2%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "batch_size = 128\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                    transform=transforms.Compose([\n",
    "                         transforms.RandomApply([transforms.CenterCrop(22)], p=0.1),\n",
    "                         transforms.Resize((28, 28)),\n",
    "                         transforms.RandomRotation((-15., 15.)),\n",
    "                        # transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader)\n",
    "    for batch_idx, (data, target) in enumerate(pbar):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        #loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "model = Network().to(device)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(0, 20):\n",
    "    print(epoch+1)\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h_Cx9q2QFgM7"
   },
   "outputs": [],
   "source": [
    "# The Base file provided\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, 3, padding=1)      #input:28x28 Output:28x28 RF:3      #input: Output: RF:\n",
    "        self.conv2 = nn.Conv2d(8, 16, 3, padding=1)     #input:28x28x8 Output:28x28x16 RF:5\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)                 #input:28x28x16 Output:14x14x16 RF: 10\n",
    "        self.conv3 = nn.Conv2d(16, 32, 3, padding=1)   #input: Output: RF:\n",
    "        self.conv4 = nn.Conv2d(32, 64, 3, padding=1)  #input: Output: RF:\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)                 #input: Output: RF:\n",
    "        self.conv5 = nn.Conv2d(256, 512, 3)             #input: Output: RF:\n",
    "        self.conv6 = nn.Conv2d(512, 1024, 3)            #input: Output: RF:\n",
    "        self.conv7 = nn.Conv2d(1024, 10, 3)             #input: Output: RF:\n",
    "\n",
    "        self.fc1 = nn.Linear(16*7*7, 10)\n",
    "        #self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x)))))\n",
    "        x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n",
    "        x = F.relu(self.conv6(F.relu(self.conv5(x))))\n",
    "        x = F.relu(self.conv7(x))\n",
    "        x = x.view(-1, 10)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xdydjYTZFyi3",
    "outputId": "a0bd0211-b3aa-433d-d977-5044380ee37d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 8, 28, 28]              80\n",
      "         MaxPool2d-2            [-1, 8, 14, 14]               0\n",
      "            Conv2d-3           [-1, 16, 14, 14]           1,168\n",
      "         MaxPool2d-4             [-1, 16, 7, 7]               0\n",
      "            Linear-5                   [-1, 10]           7,850\n",
      "================================================================\n",
      "Total params: 9,098\n",
      "Trainable params: 9,098\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.09\n",
      "Params size (MB): 0.03\n",
      "Estimated Total Size (MB): 0.13\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary\n",
    "from torchsummary import summary\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "model = Net().to(device)\n",
    "summary(model, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqTWLaM5GHgH"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "torch.manual_seed(1)\n",
    "batch_size = 128\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                    transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8fDefDhaFlwH"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader)\n",
    "    for batch_idx, (data, target) in enumerate(pbar):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MMWbLWO6FuHb",
    "outputId": "16e1a86d-97ac-4fbb-8dad-a166fe0abbb2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=nan batch_id=468: 100%|██████████| 469/469 [00:18<00:00, 25.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: nan, Accuracy: 980/10000 (10%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "for epoch in range(1, 2):\n",
    "    print(epoch)\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "So5uk4EkHW6R"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
